{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификация запрещенных комментариев \n",
    "\n",
    "Этот ноутбук основан на одном из заданий [ШАД](https://github.com/yandexdataschool/nlp_course/blob/2023/week02_classification/homework_part1.ipynb)\n",
    "\n",
    "![img](https://github.com/yandexdataschool/nlp_course/raw/master/resources/banhammer.jpg)\n",
    "\n",
    "__В этом ноутбуке__ вы построите алгоритм который классифицирует комментарии на нормальные или токсичные.\n",
    "Как и во многих реальных случаях, у вас есть только небольшой (10 ^ 3) набор данных с примерами, помеченными вручную, для работы. Мы решим эту проблему, используя классические методы nlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>should_ban</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Those who're in advantageous positions are th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>1</td>\n",
       "      <td>Fartsalot56 says f**k you motherclucker!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>1</td>\n",
       "      <td>Are you a fool? \\n\\nI am sorry, but you seem t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>1</td>\n",
       "      <td>I AM NOT A VANDAL!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>0</td>\n",
       "      <td>Citing sources\\n\\nCheck out the Wikipedia:Citi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     should_ban                                       comment_text\n",
       "50            0  \"Those who're in advantageous positions are th...\n",
       "250           1          Fartsalot56 says f**k you motherclucker!!\n",
       "450           1  Are you a fool? \\n\\nI am sorry, but you seem t...\n",
       "650           1    I AM NOT A VANDAL!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
       "850           0  Citing sources\\n\\nCheck out the Wikipedia:Citi..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"comments.tsv\", sep='\\t')\n",
    "\n",
    "texts = data['comment_text'].values\n",
    "target = data['should_ban'].values\n",
    "data[50::200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "texts_train, texts_test, y_train, y_test = train_test_split(texts, target, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Примечание:__ как правило, хорошей идеей является разделение данных на обучающие / тестовые, прежде чем что-либо с ними делать.\n",
    "\n",
    "Это защищает вас от возможной утечки данных на этапе предварительной обработки. Например, если вы решите выбрать слова, присутствующие в непристойных твитах, в качестве функций, вам следует учитывать только эти слова в обучающем наборе. В противном случае ваш алгоритм может обмануть оценку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and tokenization\n",
    "\n",
    "Комментарии содержат необработанный текст с пунктуацией, заглавными/строчными буквами и даже символами новой строки.\n",
    "\n",
    "Чтобы упростить все дальнейшие шаги, мы разделим текст на токены, разделенные пробелами, используя один из токенизаторов nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: How to be a grown-up at work: replace \"shit\" with \"Ok, great!\".\n",
      "after: how to be a grown-up at work : replace \" shit \" with \" ok , great ! \" .\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "preprocess = lambda text: ' '.join(tokenizer.tokenize(text.lower()))\n",
    "\n",
    "text = 'How to be a grown-up at work: replace \"shit\" with \"Ok, great!\".'\n",
    "print(\"before:\", text,)\n",
    "print(\"after:\", preprocess(text),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task: Preprocess каждый твит в обучающем и тестовом множестве\n",
    "\n",
    "texts_train = [preprocess(text) for text in texts_train]\n",
    "texts_test = [preprocess(text) for text in texts_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert texts_train[5] ==  'who cares anymore . they attack with impunity .'\n",
    "assert texts_test[89] == 'hey todds ! quick q ? why are you so gay'\n",
    "assert len(texts_test) == len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решаем: bag of words\n",
    "\n",
    "![img](http://www.novuslight.com/uploads/n/BagofWords.jpg)\n",
    "\n",
    "Одним из традиционных подходов к решению такой проблемы является использование фичей bag of words:\n",
    "1. создайте словарь часто употребляемых слов (используйте только данные из обучения)\n",
    "2. для каждой обучающей выборки подсчитайте, сколько раз в ней встречается слово (для каждого слова в словарном запасе).\n",
    "3. рассматривайте этот подсчет как фичи для некоторого классификатора\n",
    "\n",
    "__Note:__ in practice, you can compute such features using sklearn. Please don't do that in the current assignment, though.\n",
    "* `from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example features: ['!', '14:54', '430', 'accidental', \"ain't\", 'apologise', 'attention', 'begs', 'botticelli', 'care', 'civil', 'concerning', 'cossacks', 'day', 'determined', 'dogs', 'eh', 'every', 'fatuorum', 'forgotten', 'german', 'hahahahahaha', 'hist', 'imperial', 'intervening', 'justification', 'least', 'look', \"mclaren's\", 'moore', 'neva', 'obvious', 'pair', 'pieces', 'preconceptions', 'puff', 'recliner', 'researchers', 'rumours', 'sending', 'sincerly', 'spam', 'struggle', 'tagged', 'thing', 'treasury', 'unless', 'visited', 'whomever', 'xizer']\n"
     ]
    }
   ],
   "source": [
    "# task: Найдите до k самых часто встречающихся токенов в тексатх обучающей выборки\n",
    "# Отсортируйте их по количеству вхождений\n",
    "k = 5000\n",
    "\n",
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "for text in texts_train:\n",
    "    cnt.update(text.split())\n",
    "\n",
    "bow_vocabulary = [w[0] for w in cnt.most_common(k)]\n",
    "\n",
    "print('example features:', sorted(bow_vocabulary)[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_bow(text):\n",
    "    \"\"\" convert text string to an array of token counts. Use bow_vocabulary. \"\"\"\n",
    "    ans = np.zeros(k)\n",
    "    for w in text.split():\n",
    "        if (w in bow_vocabulary):\n",
    "            ans[bow_vocabulary.index(w)] += 1\n",
    "    return np.array(ans, 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow = np.stack(list(map(text_to_bow, texts_train)))\n",
    "X_test_bow = np.stack(list(map(text_to_bow, texts_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_max = len(set(' '.join(texts_train).split()))\n",
    "assert X_train_bow.shape == (len(texts_train), min(k, k_max))\n",
    "assert X_test_bow.shape == (len(texts_test), min(k, k_max))\n",
    "assert np.all(X_train_bow[5:10].sum(-1) == np.array([len(s.split()) for s in  texts_train[5:10]]))\n",
    "assert len(bow_vocabulary) <= min(k, k_max)\n",
    "assert X_train_bow[6, bow_vocabulary.index('.')] == texts_train[6].split().count('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Naive bayes:__ возможно, самой простой моделью, которая может решить вашу проблему, является так называемый наивный байесовский классификатор. \n",
    "Это тривиальная линейная модель, которая предполагает независимость входных фичей и вычисляет коэффициенты путем, ???, подсчета вероятностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryNaiveBayes:\n",
    "    delta = 1.0  # добавляется чтобы сгладить вероятности\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit a NaiveBayes classifier for two classes\n",
    "        :param X: [batch_size, vocab_size] of bag-of-words features\n",
    "        :param y: [batch_size] of binary targets {0, 1}\n",
    "        \"\"\"\n",
    "        # посчитайте обычные вероятности классов, p(y=k) for k = 0,1\n",
    "        self.p_y = np.array(<YOUR CODE: probability of y=0 and of y=1 in that order>)\n",
    "        \n",
    "        # посчитайте количество вхождений каждого слова в тексты с метками 0 и 1 раздельно\n",
    "        word_counts_positive = <YOUR CODE HERE>\n",
    "        word_counts_negative = <YOUR CODE HERE>\n",
    "        # должны быть размера [vocab_size].\n",
    "        \n",
    "        # Наконец, используйте эти вектора чтобы оценить p(x | y = k) для k = 0, 1\n",
    "        \n",
    "        <YOUR CODE HERE>\n",
    "        self.p_x_given_positive = <...>\n",
    "        self.p_x_given_negative = <...>\n",
    "        # должен быть размера [vocab_size]; не забудьте добавить self.delta!\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_scores(self, X):\n",
    "        \"\"\"\n",
    "        :param X: [batch_size, vocab_size] of bag-of-words features\n",
    "        :returns: a matrix of scores [batch_size, k] of scores for k-th class\n",
    "        \"\"\"\n",
    "        # compute scores for positive and negative classes separately.\n",
    "        # these scores should be proportional to log-probabilities of the respective target {0, 1}\n",
    "        # note: if you apply logarithm to p_x_given_*, the total log-probability can be written\n",
    "        # as a dot-product with X\n",
    "        score_negative = X.dot(np.log(self.p_x_given_negative))\n",
    "        score_positive = X.dot(np.log(self.p_x_given_positive))\n",
    "        \n",
    "        # you can compute total p(x | y=k) with a dot product\n",
    "        return np.stack([score_negative, score_positive], axis=-1)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.predict_scores(X).argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_model = BinaryNaiveBayes().fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert naive_model.p_y.shape == (2,) and naive_model.p_y.sum() == 1 and naive_model.p_y[0] > naive_model.p_y[1]\n",
    "assert naive_model.p_x_given_positive.shape == naive_model.p_x_given_negative.shape == X_train_bow.shape[1:]\n",
    "assert np.allclose(naive_model.p_x_given_positive.sum(), 1.0)\n",
    "assert np.allclose(naive_model.p_x_given_negative.sum(), 1.0)\n",
    "assert naive_model.p_x_given_negative.min() > 0, \"did you forget to add delta?\"\n",
    "\n",
    "f_index = bow_vocabulary.index('fuck')  # offensive tweets should contain more of this\n",
    "assert naive_model.p_x_given_positive[f_index] > naive_model.p_x_given_negative[f_index]\n",
    "\n",
    "g_index = bow_vocabulary.index('good')  # offensive tweets should contain less of this\n",
    "assert naive_model.p_x_given_positive[g_index] < naive_model.p_x_given_negative[g_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "for name, X, y, model in [\n",
    "    ('train', X_train_bow, y_train, naive_model),\n",
    "    ('test ', X_test_bow, y_test, naive_model)\n",
    "]:\n",
    "    proba = model.predict_scores(X)[:, 1] - model.predict_scores(X)[:, 0]\n",
    "    auc = roc_auc_score(y, proba)\n",
    "    plt.plot(*roc_curve(y, proba)[:2], label='%s AUC=%.4f' % (name, auc))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], '--', color='black',)\n",
    "plt.legend(fontsize='large')\n",
    "plt.grid()\n",
    "\n",
    "test_accuracy = np.mean(naive_model.predict(X_test_bow) == y_test)\n",
    "print(f\"Model accuracy: {test_accuracy:.3f}\")\n",
    "assert test_accuracy > 0.75, \"Accuracy too low. There's likely a mistake in the code.\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ладно, он определенно научился *чему-то*. Теперь давайте разберемся, чему именно он научился. Самый простой способ сделать это - выделить, какие слова имеют наибольшее соотношение положительной и отрицательной вероятности или наоборот. Мы остановимся на первом.\n",
    "\n",
    "__Ваша задача__ это подсчитать top-25 слов у которых __наибольшее__ соотношение ${p(x_i | y=1)} \\over {p(x_i | y=0)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint: use naive_model.p_*\n",
    "probability_ratio = <YOUR CODE: compute the ratio as defined above, must be a vector of [vocab_size]>\n",
    "top_negative_words = <YOUR CODE: find 25 words with highest probability_ratio, return list of str>\n",
    "\n",
    "assert len(top_negative_words) == 25 and [isinstance(w, str) for w in top_negative_words]\n",
    "assert 'j.delanoy' in top_negative_words and 'college' in top_negative_words\n",
    "\n",
    "for i, word in enumerate(top_negative_words):\n",
    "    print(f\"#{i}\\t{word.rjust(10, ' ')}\\t(ratio={probability_ratio[bow_vocabulary.index(word)]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте попробуем что-нибудь менее доисторическое: __Логистическая регрессия__. Вы можете найти веса модели, оптимизировав логарифмическую вероятность ответа. Хотя, конечно, вам даже не нужно больше писать это от руки. Давайте обучим это!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "bow_model = <YOUR CODE HERE - train a logistic regression>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "for name, X, y, model in [\n",
    "    ('train', X_train_bow, y_train, bow_model),\n",
    "    ('test ', X_test_bow, y_test, bow_model)\n",
    "]:\n",
    "    proba = model.predict_proba(X)[:, 1]\n",
    "    auc = roc_auc_score(y, proba)\n",
    "    plt.plot(*roc_curve(y, proba)[:2], label='%s AUC=%.4f' % (name, auc))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], '--', color='black',)\n",
    "plt.legend(fontsize='large')\n",
    "plt.grid()\n",
    "\n",
    "test_accuracy = np.mean(bow_model.predict(X_test_bow) == y_test)\n",
    "print(f\"Model accuracy: {test_accuracy:.3f}\")\n",
    "assert test_accuracy > 0.77, \"Hint: tune the parameter C to improve performance\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: implement TF-IDF features\n",
    "\n",
    "Not all words are equally useful. One can prioritize rare words and downscale words like \"and\"/\"or\" by using __tf-idf features__. This abbreviation stands for __text frequency/inverse document frequence__ and means exactly that:\n",
    "\n",
    "$$ feature_i = { Count(word_i \\in x) \\times { log {N \\over Count(word_i \\in D) + \\alpha} }} $$\n",
    "\n",
    "\n",
    ", where x is a single text, D is your dataset (a collection of texts), N is a total number of documents and $\\alpha$ is a smoothing hyperparameter (typically 1). \n",
    "And $Count(word_i \\in D)$ is the number of documents where $word_i$ appears.\n",
    "\n",
    "It may also be a good idea to normalize each data sample after computing tf-idf features.\n",
    "\n",
    "__Your task:__ implement tf-idf features, train a model and evaluate ROC curve. Compare it with basic BagOfWords model from above.\n",
    "\n",
    "Please don't use sklearn/nltk builtin tf-idf vectorizers in your solution :) You can still use 'em for debugging though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
